{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://www.kaggle.com/competitions/kaggle-llm-science-exam"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:14:01.342476Z","iopub.status.busy":"2023-10-10T12:14:01.341932Z","iopub.status.idle":"2023-10-10T12:14:39.358195Z","shell.execute_reply":"2023-10-10T12:14:39.356934Z","shell.execute_reply.started":"2023-10-10T12:14:01.342442Z"},"trusted":true},"outputs":[],"source":["!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n","!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n","!cp /kaggle/input/util-openbook-2-py/util_openbook_2.py ."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:14:39.361127Z","iopub.status.busy":"2023-10-10T12:14:39.360673Z","iopub.status.idle":"2023-10-10T12:16:40.389690Z","shell.execute_reply":"2023-10-10T12:16:40.388473Z","shell.execute_reply.started":"2023-10-10T12:14:39.361089Z"},"trusted":true},"outputs":[],"source":["# installing offline dependencies\n","!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n","!pip install -U /kaggle/working/sentence-transformers\n","!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n","\n","!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n","!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n","!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:16:40.392007Z","iopub.status.busy":"2023-10-10T12:16:40.391618Z","iopub.status.idle":"2023-10-10T12:25:56.926776Z","shell.execute_reply":"2023-10-10T12:25:56.925840Z","shell.execute_reply.started":"2023-10-10T12:16:40.391971Z"},"trusted":true},"outputs":[],"source":["from util_openbook_2 import get_contexts, generate_openbook_output\n","import pickle\n","\n","get_contexts()\n","generate_openbook_output()\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-10T12:25:56.929818Z","iopub.status.busy":"2023-10-10T12:25:56.929409Z","iopub.status.idle":"2023-10-10T12:25:56.947488Z","shell.execute_reply":"2023-10-10T12:25:56.946581Z","shell.execute_reply.started":"2023-10-10T12:25:56.929793Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","from datasets import load_dataset, load_from_disk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import torch\n","from transformers import LongformerTokenizer, LongformerForMultipleChoice\n","import transformers\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import unicodedata\n","\n","import os\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForMultipleChoice"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:25:56.951286Z","iopub.status.busy":"2023-10-10T12:25:56.950604Z","iopub.status.idle":"2023-10-10T12:26:20.900054Z","shell.execute_reply":"2023-10-10T12:26:20.898527Z","shell.execute_reply.started":"2023-10-10T12:25:56.951253Z"},"trusted":true},"outputs":[],"source":["!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n","!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:26:20.902789Z","iopub.status.busy":"2023-10-10T12:26:20.902150Z","iopub.status.idle":"2023-10-10T12:26:20.925551Z","shell.execute_reply":"2023-10-10T12:26:20.924627Z","shell.execute_reply.started":"2023-10-10T12:26:20.902751Z"},"trusted":true},"outputs":[],"source":["def SplitList(mylist, chunk_size):\n","    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n","\n","def get_relevant_documents_parsed(df_valid):\n","    df_chunk_size=600\n","    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n","    modified_texts = paraphs_parsed_dataset.map(lambda example:\n","                                             {'temp_text':\n","                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         paraphs_parsed_dataset[idx.item()][\"title\"],\n","                         paraphs_parsed_dataset[idx.item()][\"text\"],\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def get_relevant_documents(df_valid):\n","    df_chunk_size=800\n","    \n","    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n","    modified_texts = cohere_dataset_filtered.map(lambda example:\n","                                             {'temp_text':\n","                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         cohere_dataset_filtered[idx.item()][\"title\"],\n","                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def retrieval(df_valid, modified_texts):\n","    \n","    corpus_df_valid = df_valid.apply(lambda row:\n","                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n","                                     axis=1).values\n","    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words)\n","    vectorizer1.fit(corpus_df_valid)\n","    vocab_df_valid = vectorizer1.get_feature_names_out()\n","    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words,\n","                                 vocabulary=vocab_df_valid)\n","    vectorizer.fit(modified_texts[:500000])\n","    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n","    \n","    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n","\n","    chunk_size = 100000\n","    top_per_chunk = 10\n","    top_per_query = 10\n","\n","    all_chunk_top_indices = []\n","    all_chunk_top_values = []\n","\n","    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n","        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n","        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n","        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n","        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n","\n","        all_chunk_top_indices.append(chunk_top_indices + idx)\n","        all_chunk_top_values.append(chunk_top_values)\n","\n","    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n","    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n","    \n","    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n","    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n","    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n","    \n","    return articles_indices, merged_top_scores\n","\n","\n","def prepare_answering_input(\n","        tokenizer, \n","        question,  \n","        options,   \n","        context,   \n","        max_seq_length=4096,\n","    ):\n","    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n","    c_plus_q_4 = [c_plus_q] * len(options)\n","    tokenized_examples = tokenizer(\n","        c_plus_q_4, options,\n","        max_length=max_seq_length,\n","        padding=\"longest\",\n","        truncation=False,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n","    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n","    example_encoded = {\n","        \"input_ids\": input_ids.to(model.device.index),\n","        \"attention_mask\": attention_mask.to(model.device.index),\n","    }\n","    return example_encoded\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:26:20.928056Z","iopub.status.busy":"2023-10-10T12:26:20.927407Z","iopub.status.idle":"2023-10-10T12:26:20.948340Z","shell.execute_reply":"2023-10-10T12:26:20.947163Z","shell.execute_reply.started":"2023-10-10T12:26:20.928023Z"},"trusted":true},"outputs":[],"source":["stop_words = ['each', 'you', 'the', 'use', 'used',\n","                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n","                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n","                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n","                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n","                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n","                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n","                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n","                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n","                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n","                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n","                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n","                  'did', 'theirs', 'can', 'those',\n","                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n","                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n","                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n","                  'yours', 'but', 'being', \"wasn't\", 'be']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:26:20.950565Z","iopub.status.busy":"2023-10-10T12:26:20.949897Z","iopub.status.idle":"2023-10-10T12:26:20.978886Z","shell.execute_reply":"2023-10-10T12:26:20.978048Z","shell.execute_reply.started":"2023-10-10T12:26:20.950533Z"},"trusted":true},"outputs":[],"source":["df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:26:20.980830Z","iopub.status.busy":"2023-10-10T12:26:20.980305Z","iopub.status.idle":"2023-10-10T12:33:59.280965Z","shell.execute_reply":"2023-10-10T12:33:59.279857Z","shell.execute_reply.started":"2023-10-10T12:26:20.980799Z"},"trusted":true},"outputs":[],"source":["retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:33:59.284718Z","iopub.status.busy":"2023-10-10T12:33:59.284005Z","iopub.status.idle":"2023-10-10T12:43:08.609669Z","shell.execute_reply":"2023-10-10T12:43:08.608561Z","shell.execute_reply.started":"2023-10-10T12:33:59.284660Z"},"trusted":true},"outputs":[],"source":["retrieved_articles = get_relevant_documents(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:43:08.614781Z","iopub.status.busy":"2023-10-10T12:43:08.613897Z","iopub.status.idle":"2023-10-10T12:43:25.069184Z","shell.execute_reply":"2023-10-10T12:43:25.068192Z","shell.execute_reply.started":"2023-10-10T12:43:08.614738Z"},"trusted":true},"outputs":[],"source":["#Longformer\n","tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\n","model = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:43:25.070845Z","iopub.status.busy":"2023-10-10T12:43:25.070458Z","iopub.status.idle":"2023-10-10T12:48:11.974559Z","shell.execute_reply":"2023-10-10T12:48:11.973617Z","shell.execute_reply.started":"2023-10-10T12:43:25.070809Z"},"trusted":true},"outputs":[],"source":["predictions = []\n","submit_ids = []\n","\n","for index in tqdm(range(df_valid.shape[0])):\n","    columns = df_valid.iloc[index].values\n","    submit_ids.append(columns[0])\n","    question = columns[1]\n","    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n","    context1 = context1[:2000]\n","    context2 = context2[:2000]\n","    \n","    inputs1 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context1\n","    )\n","    inputs2 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context2\n","    )\n","    \n","    del columns,question,options,context1,context2\n","\n","    with torch.no_grad():\n","        outputs1 = model(**inputs1)    \n","        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n","        del outputs1\n","        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n","\n","        outputs2 = model(**inputs2)\n","        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n","        del outputs2\n","        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n","\n","    probability_ = (probability1 + probability2) / 2\n","    \n","    predictions.append(probability_)\n","    del probability_,probability1,probability2\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:48:11.976538Z","iopub.status.busy":"2023-10-10T12:48:11.976190Z","iopub.status.idle":"2023-10-10T12:48:12.307103Z","shell.execute_reply":"2023-10-10T12:48:12.306004Z","shell.execute_reply.started":"2023-10-10T12:48:11.976505Z"},"trusted":true},"outputs":[],"source":["backup_model_predictions_2=pd.DataFrame({'id':submit_ids,'prediction':predictions})\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:48:12.308937Z","iopub.status.busy":"2023-10-10T12:48:12.308580Z","iopub.status.idle":"2023-10-10T12:48:29.541178Z","shell.execute_reply":"2023-10-10T12:48:29.540231Z","shell.execute_reply.started":"2023-10-10T12:48:12.308906Z"},"trusted":true},"outputs":[],"source":["#llm-science-run-context-2\n","tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llm-science-run-context-2\")\n","model = AutoModelForMultipleChoice.from_pretrained(\"/kaggle/input/llm-science-run-context-2\").cuda()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:48:29.543378Z","iopub.status.busy":"2023-10-10T12:48:29.542729Z","iopub.status.idle":"2023-10-10T12:51:19.629929Z","shell.execute_reply":"2023-10-10T12:51:19.628951Z","shell.execute_reply.started":"2023-10-10T12:48:29.543344Z"},"trusted":true},"outputs":[],"source":["predictions = []\n","submit_ids = []\n","\n","for index in tqdm(range(df_valid.shape[0])):\n","    columns = df_valid.iloc[index].values\n","    submit_ids.append(columns[0])\n","    question = columns[1]\n","    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n","    \n","    inputs1 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context1\n","    )\n","    inputs2 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context2\n","    )\n","    \n","    del columns,question,options,context1,context2\n","\n","    with torch.no_grad():\n","        outputs1 = model(**inputs1)    \n","        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n","        del outputs1\n","        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n","\n","        outputs2 = model(**inputs2)\n","        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n","        del outputs2\n","        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n","\n","    probability_ = (probability1 + probability2) / 2\n","    \n","    predictions.append(probability_)\n","    del probability_,probability1,probability2\n","    \n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:51:19.632193Z","iopub.status.busy":"2023-10-10T12:51:19.631513Z","iopub.status.idle":"2023-10-10T12:51:19.979094Z","shell.execute_reply":"2023-10-10T12:51:19.978089Z","shell.execute_reply.started":"2023-10-10T12:51:19.632159Z"},"trusted":true},"outputs":[],"source":["backup_model_predictions_3=pd.DataFrame({'id':submit_ids,'prediction':predictions})\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:51:19.981420Z","iopub.status.busy":"2023-10-10T12:51:19.980464Z","iopub.status.idle":"2023-10-10T12:51:19.998350Z","shell.execute_reply":"2023-10-10T12:51:19.997473Z","shell.execute_reply.started":"2023-10-10T12:51:19.981382Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","backup_model_predictions = pd.read_csv(\"submission_backup.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:51:20.000040Z","iopub.status.busy":"2023-10-10T12:51:19.999721Z","iopub.status.idle":"2023-10-10T12:51:35.906072Z","shell.execute_reply":"2023-10-10T12:51:35.905190Z","shell.execute_reply.started":"2023-10-10T12:51:20.000009Z"},"trusted":true},"outputs":[],"source":["#debertav3\n","debertav3_model_dir = \"/kaggle/input/llm-se-debertav3-large\"\n","tokenizer = AutoTokenizer.from_pretrained(debertav3_model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(debertav3_model_dir).cuda()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:51:35.911843Z","iopub.status.busy":"2023-10-10T12:51:35.909847Z","iopub.status.idle":"2023-10-10T12:53:47.805594Z","shell.execute_reply":"2023-10-10T12:53:47.804567Z","shell.execute_reply.started":"2023-10-10T12:51:35.911808Z"},"trusted":true},"outputs":[],"source":["predictions = []\n","submit_ids = []\n","\n","def convert_str_to_tensor_v2(s):\n","    # Remove 'tensor(', ')' and square brackets from the string\n","    s = s.replace('tensor(', '').replace(')', '').replace('[', '').replace(']', '')\n","    \n","    # Convert string to list of floats\n","    float_list = [float(num) for num in s.split(',')]\n","    \n","    # Convert list to tensor\n","    tensor_val = torch.tensor(float_list)\n","    \n","    return tensor_val\n","\n","\n","for index in tqdm(range(df_valid.shape[0])):\n","    columns = df_valid.iloc[index].values\n","    submit_ids.append(columns[0])\n","    question = columns[1]\n","    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n","    \n","    probabilities = []\n","\n","    inputs1 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context1\n","    )\n","    inputs2 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context2\n","    )\n","    \n","    del columns,question,options,context1,context2\n","\n","    with torch.no_grad():\n","        outputs1 = model(**inputs1)    \n","        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n","        del outputs1\n","        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n","\n","        outputs2 = model(**inputs2)\n","        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n","        del outputs2\n","        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n","\n","    model_probability = (probability1 + probability2) / 2\n","    probabilities.append(model_probability)\n","\n","    #backup_model\n","    tensor_str=backup_model_predictions.iloc[index].prediction\n","    backup_model_probability=convert_str_to_tensor_v2(tensor_str)\n","    probabilities.append(backup_model_probability)\n","    \n","    #backup_model_2\n","    probabilities.append(backup_model_predictions_2.iloc[index].prediction)\n","    \n","    #backup_model_3\n","    probabilities.append(backup_model_predictions_3.iloc[index].prediction)\n","    \n","    probability_ = sum(probabilities) / len(probabilities)\n","    \n","    \n","    predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n","    del probability_,probability1,probability2\n","    \n","    predictions.append(predict)\n","\n","predictions = [\" \".join(i) for i in predictions]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:53:47.808034Z","iopub.status.busy":"2023-10-10T12:53:47.807065Z","iopub.status.idle":"2023-10-10T12:53:47.817026Z","shell.execute_reply":"2023-10-10T12:53:47.816209Z","shell.execute_reply.started":"2023-10-10T12:53:47.808000Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame({'id':submit_ids,'prediction':predictions}).to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:53:47.819434Z","iopub.status.busy":"2023-10-10T12:53:47.818730Z","iopub.status.idle":"2023-10-10T12:53:48.221682Z","shell.execute_reply":"2023-10-10T12:53:48.220719Z","shell.execute_reply.started":"2023-10-10T12:53:47.819387Z"},"trusted":true},"outputs":[],"source":["#delete file\n","import os\n","def delete_files_except_submission_csv(dir_path):\n","    for root, dirs, files in os.walk(dir_path, topdown=False):\n","        for file in files:\n","            if file != 'submission.csv':\n","                file_path = os.path.join(root, file)\n","                os.remove(file_path)\n","        for dir in dirs:\n","            dir_path = os.path.join(root, dir)\n","            os.rmdir(dir_path)\n","working_dir = '/kaggle/working'\n","delete_files_except_submission_csv(working_dir)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
